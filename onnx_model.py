import torch
import os
import time
import tempfile
import shutil
import onnxruntime as ort
import numpy as np
from diffusers import FluxPipeline
from torch.onnx import register_custom_op_symbolic
import onnx
from onnx.external_data_helper import convert_model_to_external_data

# ç²¾åº¦é…ç½®
# æ³¨æ„:
# - fp32/fp16/bf16: æ ‡å‡†ç²¾åº¦ï¼ŒONNX Runtime å®Œå…¨æ”¯æŒ
# - fp8/fp4: éœ€è¦ç‰¹æ®Šçš„é‡åŒ–å·¥å…·é“¾ï¼ŒONNX Runtime åŸç”Ÿä¸æ”¯æŒ
#   - FP8: éœ€è¦ NVIDIA Transformer Engine æˆ– TensorRT 9+ (Hopper GPU)
#   - FP4/INT4: éœ€è¦ä½¿ç”¨ GPTQ/AWQ ç­‰é‡åŒ–æ–¹æ³•ï¼Œç„¶åç”¨ä¸“é—¨çš„æ¨ç†å¼•æ“
PRECISION_MAP = {
    "fp32": {"torch_dtype": torch.float32, "np_dtype": np.float32},
    "fp16": {"torch_dtype": torch.float16, "np_dtype": np.float16},
    "bf16": {"torch_dtype": torch.bfloat16, "np_dtype": np.float16},
    # FP8/FP4 éœ€è¦é¢å¤–çš„é‡åŒ–æ­¥éª¤ï¼Œè¿™é‡Œä»…ä½œæ ‡è®°
    # "fp8": éœ€è¦ Transformer Engine é‡åŒ–åå¯¼å‡º
    # "fp4": éœ€è¦ GPTQ/AWQ é‡åŒ–åå¯¼å‡º
}


def get_output_path(output_dir: str, model_name: str, precision: str) -> str:
    """æ ¹æ®ç²¾åº¦ç”Ÿæˆè¾“å‡ºè·¯å¾„"""
    return os.path.join(output_dir, f"{model_name}_{precision}.onnx")
# å®šä¹‰ rms_norm çš„ç¬¦å·åŒ–å‡½æ•°
def rms_norm_symbolic(g, input, normalized_shape, weight, eps):
    """
    å°† rms_norm åˆ†è§£ä¸º ONNX æ”¯æŒçš„åŸºç¡€æ“ä½œ
    RMSNorm(x) = x / RMS(x) * weight
    å…¶ä¸­ RMS(x) = sqrt(mean(x^2) + eps)
    """
    # è®¡ç®— x^2
    square = g.op("Mul", input, input)
    
    # è®¡ç®— mean(x^2)ï¼Œåœ¨æœ€åä¸€ä¸ªç»´åº¦ä¸Š
    mean = g.op("ReduceMean", square, axes_i=[-1], keepdims_i=1)
    
    # åŠ ä¸Š epsï¼ˆå¦‚æœ eps æ˜¯ç¬¦å·å€¼ï¼Œç›´æ¥ä½¿ç”¨ï¼›å¦‚æœæ˜¯å¸¸é‡ï¼Œéœ€è¦è½¬æ¢ï¼‰
    mean_eps = g.op("Add", mean, eps)
    
    # è®¡ç®— sqrt
    rms = g.op("Sqrt", mean_eps)
    
    # x / rms
    normalized = g.op("Div", input, rms)
    
    # * weight
    if weight is not None:
        result = g.op("Mul", normalized, weight)
    else:
        result = normalized
    
    return result

# æ³¨å†Œè‡ªå®šä¹‰ç¬¦å·åŒ–å‡½æ•°
register_custom_op_symbolic('aten::rms_norm', rms_norm_symbolic, opset_version=17)


def export_transformer_to_onnx(model_path: str, output_dir: str, model_name: str, precision: str):
    """å¯¼å‡º FLUX Transformer åˆ° ONNX æ ¼å¼"""
    if precision not in PRECISION_MAP:
        raise ValueError(f"Unsupported precision: {precision}. Supported: {list(PRECISION_MAP.keys())}")
    
    dtype = PRECISION_MAP[precision]["torch_dtype"]
    output_path = get_output_path(output_dir, model_name, precision)
    
    print("=" * 80)
    print(f"Exporting FLUX Transformer to ONNX ({precision})")
    print("=" * 80)
    print(f"Model path: {model_path}")
    print(f"Output path: {output_path}")
    print("-" * 80)
    
    # 1. åŠ è½½ Transformer ç»„ä»¶
    print("Loading FLUX pipeline...")
    start_time = time.time()
    
    load_dtype = torch.bfloat16 if precision == "bf16" else dtype
    pipe = FluxPipeline.from_pretrained(model_path, torch_dtype=load_dtype)
    transformer = pipe.transformer
    transformer.eval()
    
    # bf16 è½¬æ¢ä¸º fp16 å¯¼å‡ºï¼ˆONNX Runtime å¯¹ bf16 æ”¯æŒæœ‰é™ï¼‰
    if precision == "bf16":
        transformer = transformer.to(torch.float16)
    
    transformer = transformer.cpu()
    
    load_time = time.time() - start_time
    print(f"Model loaded in {load_time:.2f}s")
    print("-" * 80)
    
    # 2. æ„é€  Dummy Input
    print("Preparing dummy inputs...")
    export_dtype = torch.float16 if precision in ["fp16", "bf16"] else torch.float32
    
    # 1024x1024 å›¾ç‰‡çš„ latent: 128x128ï¼Œpacked åçº¦ 4096 tokens
    hidden_states = torch.randn(1, 4096, 64, dtype=export_dtype)
    encoder_hidden_states = torch.randn(1, 512, 4096, dtype=export_dtype)
    pooled_projections = torch.randn(1, 768, dtype=export_dtype)
    timestep = torch.tensor([1.0], dtype=export_dtype)
    img_ids = torch.randn(4096, 3, dtype=export_dtype)
    txt_ids = torch.randn(512, 3, dtype=export_dtype)
    guidance = torch.tensor([3.5], dtype=export_dtype)

    dummy_inputs = (
        hidden_states,
        encoder_hidden_states,
        pooled_projections,
        timestep,
        img_ids,
        txt_ids,
        guidance
    )
    
    input_names = [
        "hidden_states", 
        "encoder_hidden_states", 
        "pooled_projections", 
        "timestep", 
        "img_ids", 
        "txt_ids", 
        "guidance"
    ]
    output_names = ["sample"]

    # 3. å®šä¹‰åŠ¨æ€è½´
    dynamic_axes = {
        "hidden_states": {0: "batch", 1: "seq_len"},
        "encoder_hidden_states": {0: "batch", 1: "text_seq_len"},
        "pooled_projections": {0: "batch"},
        "timestep": {0: "batch"},
        "sample": {0: "batch", 1: "seq_len"}
    }

    # 4. å¯¼å‡º ONNXï¼ˆå…ˆå¯¼å‡ºåˆ°ä¸´æ—¶ç›®å½•ï¼Œé¿å…ä¸­é—´æ–‡ä»¶æ±¡æŸ“ï¼‰
    print("Exporting to ONNX (this may take several minutes)...")
    os.makedirs(output_dir, exist_ok=True)
    
    # åˆ›å»ºä¸´æ—¶ç›®å½•ä¿å­˜ä¸­é—´æ–‡ä»¶
    temp_dir = tempfile.mkdtemp(prefix="onnx_export_")
    temp_onnx_path = os.path.join(temp_dir, os.path.basename(output_path))
    
    export_start = time.time()
    
    try:
        torch.onnx.export(
            transformer,
            dummy_inputs,
            temp_onnx_path,
            export_params=True,
            opset_version=17,
            do_constant_folding=True,
            input_names=input_names,
            output_names=output_names,
            dynamic_axes=dynamic_axes,
            verbose=False,
            keep_initializers_as_inputs=False,
        )
        export_time = time.time() - export_start
        
        # å¯¹äºå¤§æ¨¡å‹ï¼Œéœ€è¦ä½¿ç”¨å¤–éƒ¨æ•°æ®æ ¼å¼
        onnx_filename = os.path.basename(output_path)
        external_data_path = onnx_filename.replace('.onnx', '_weights.bin')
        
        print("Converting to external data format (required for large models)...")
        
        # åŠ è½½æ¨¡å‹ï¼ˆåŒ…æ‹¬å·²æœ‰çš„å¤–éƒ¨æ•°æ®ï¼‰
        model = onnx.load(temp_onnx_path, load_external_data=True)
        
        # è½¬æ¢ä¸ºå•ä¸ªå¤–éƒ¨æ•°æ®æ–‡ä»¶
        convert_model_to_external_data(
            model,
            all_tensors_to_one_file=True,
            location=external_data_path,
            size_threshold=1024,
            convert_attribute=False
        )
        
        # ä¿å­˜æœ€ç»ˆæ¨¡å‹åˆ°è¾“å‡ºç›®å½•
        onnx.save_model(model, output_path)
        
        print(f"Model saved with external data: {external_data_path}")
        
    finally:
        # æ¸…ç†ä¸´æ—¶ç›®å½•
        if os.path.exists(temp_dir):
            shutil.rmtree(temp_dir)
            print(f"Cleaned up temp directory: {temp_dir}")
    
    # è·å–æ–‡ä»¶å¤§å°
    onnx_size_mb = os.path.getsize(output_path) / (1024 ** 2)
    weights_path = os.path.join(output_dir, external_data_path)
    weights_size_mb = os.path.getsize(weights_path) / (1024 ** 2) if os.path.exists(weights_path) else 0
    file_size_mb = onnx_size_mb + weights_size_mb
    
    print(f"âœ… Export successful!")
    print(f"Export time: {export_time:.2f}s")
    print(f"Precision: {precision}")
    print(f"ONNX graph size: {onnx_size_mb:.2f} MB")
    print(f"Weights file size: {weights_size_mb:.2f} MB")
    print(f"Total size: {file_size_mb:.2f} MB")
    print(f"Saved to: {output_path}")
    print("=" * 80)
    
    return output_path


def test_onnx_inference(output_dir: str, model_name: str, precision: str):
    """æµ‹è¯• ONNX æ¨¡å‹æ¨ç†"""
    onnx_path = get_output_path(output_dir, model_name, precision)
    np_dtype = np.float16 if precision in ["fp16", "bf16"] else np.float32
    
    print("=" * 80)
    print(f"Testing ONNX Inference ({precision})")
    print("=" * 80)
    print(f"Model path: {onnx_path}")
    
    # æ£€æŸ¥å¤–éƒ¨æ•°æ®æ–‡ä»¶
    onnx_dir = os.path.dirname(onnx_path) or "."
    external_data_path = os.path.join(onnx_dir, os.path.basename(onnx_path).replace('.onnx', '_weights.bin'))
    if os.path.exists(external_data_path):
        weights_size_gb = os.path.getsize(external_data_path) / (1024 ** 3)
        print(f"External weights: {external_data_path} ({weights_size_gb:.2f} GB)")
    
    providers = ['CUDAExecutionProvider', 'CPUExecutionProvider']
    print(f"Execution providers: {providers}")
    print("-" * 80)
    
    # åˆ›å»ºæ¨ç†ä¼šè¯
    print("Creating ONNX Runtime session...")
    start_time = time.time()
    
    sess_options = ort.SessionOptions()
    sess_options.graph_optimization_level = ort.GraphOptimizationLevel.ORT_ENABLE_ALL
    session = ort.InferenceSession(onnx_path, sess_options, providers=providers)
    
    print(f"Session created in {time.time() - start_time:.2f}s")
    print(f"Available providers: {session.get_providers()}")
    print("-" * 80)
    
    # å‡†å¤‡æµ‹è¯•è¾“å…¥
    print(f"Preparing test inputs (dtype: {np_dtype})...")
    inputs = {
        "hidden_states": np.random.randn(1, 4096, 64).astype(np_dtype),
        "encoder_hidden_states": np.random.randn(1, 512, 4096).astype(np_dtype),
        "pooled_projections": np.random.randn(1, 768).astype(np_dtype),
        "timestep": np.array([1.0], dtype=np_dtype),
        "img_ids": np.random.randn(4096, 3).astype(np_dtype),
        "txt_ids": np.random.randn(512, 3).astype(np_dtype),
        "guidance": np.array([3.5], dtype=np_dtype)
    }
    
    # é¢„çƒ­
    print("Warming up...")
    _ = session.run(None, inputs)
    
    # æ­£å¼æ¨ç†
    print("Running inference...")
    start_time = time.time()
    outputs = session.run(None, inputs)
    inference_time = time.time() - start_time
    
    print(f"âœ… Inference successful!")
    print(f"Inference time: {inference_time:.3f}s")
    print(f"Output shape: {outputs[0].shape}")
    print(f"Output dtype: {outputs[0].dtype}")
    print(f"Output range: [{outputs[0].min():.4f}, {outputs[0].max():.4f}]")
    print("=" * 80)
    
    return outputs


if __name__ == "__main__":
    # ==================== é…ç½®åŒºåŸŸ ====================
    REPO_ROOT = "/vepfs-d-data/q-xbyd/cv/users/zhangjinyang/models/FLUX.1-dev"
    OUTPUT_DIR = "models"
    MODEL_NAME = "flux_transformer"
    PRECISION = "bf16"  # å¯é€‰: "fp32", "fp16", "bf16" "int8"
    # =================================================
    
    print(f"Using ONNX Runtime version: {ort.__version__}")
    print(f"\nğŸš€ FLUX ONNX Export & Test (precision: {PRECISION})\n")
    
    # Step 1: å¯¼å‡º
    onnx_path = export_transformer_to_onnx(REPO_ROOT, OUTPUT_DIR, MODEL_NAME, PRECISION)
    
    # Step 2: æµ‹è¯•
    if onnx_path and os.path.exists(onnx_path):
        test_onnx_inference(OUTPUT_DIR, MODEL_NAME, PRECISION)
    
    print("\nâœ… Done!")