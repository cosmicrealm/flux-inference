# FLUX.1-dev Inference Optimization

> Export FLUX.1-dev model to ONNX format and TensorRT Engine for optimized inference

[ä¸­æ–‡](README-zh.md) | English

This project provides a complete inference pipeline for FLUX.1-dev model, including multiple inference backends and precision options.

## âœ¨ Features

- âœ… **PyTorch Baseline** - Native BF16 inference
- âœ… **ONNX Runtime** - Cross-platform inference, supports FP16/BF16
- âœ… **TensorRT** - NVIDIA GPU extreme acceleration, supports FP16/BF16
- âœ… **Complete Image Generation** - Input prompt, output image
- âœ… **Multiple Precision Support** - FP32, FP16, BF16

## ğŸ“‹ TODO

- [ ] INT8 quantization support (TensorRT)
- [ ] FP8 precision support (requires Hopper GPU)
- [ ] Dynamic resolution support
- [ ] Batch inference optimization
- [ ] Multi-GPU parallel inference
- [ ] Text Encoder / VAE TensorRT acceleration
- [ ] CUDA Graph optimization

## ğŸ–¼ï¸ Generated Results

Using the same prompt and seed, generation results from different inference backends:

**Prompt**: *"A masterpiece photo of a beautiful sunset over rugged mountains, with dramatic, fiery clouds filling the sky. In the foreground, a golden retriever and a fluffy calico cat sit side-by-side on a rocky outcrop, looking out at the view."*

| Baseline (BF16) | ONNX (FP16) | ONNX (BF16) |
|:---------------:|:-----------:|:-----------:|
| ![baseline](results/baseline_flux_bf16.png) | ![onnx_fp16](results/onnx_output_fp16.png) | ![onnx_bf16](results/onnx_output_bf16.png) |

| TensorRT (FP16) | TensorRT (BF16) |
|:---------------:|:---------------:|
| ![trt_fp16](results/tensorrt_output_fp16.png) | ![trt_bf16](results/tensorrt_output_bf16.png) |

## ğŸ“ Project Structure

```
flux-inference/
â”œâ”€â”€ base_model.py           # PyTorch baseline inference
â”œâ”€â”€ onnx_model.py           # ONNX model export
â”œâ”€â”€ onnx_infer.py           # ONNX complete image generation
â”œâ”€â”€ tensorrt_model.py       # TensorRT Engine build
â”œâ”€â”€ tensorrt_infer.py       # TensorRT complete image generation
â”œâ”€â”€ requirements.txt        # Python dependencies
â”œâ”€â”€ README.md               # This file
â”œâ”€â”€ models/                 # Exported models directory
â”‚   â”œâ”€â”€ flux_transformer_{precision}.onnx
â”‚   â”œâ”€â”€ flux_transformer_{precision}_weights.bin
â”‚   â””â”€â”€ flux_transformer_{precision}.engine
â””â”€â”€ results/                # Generated images directory
    â”œâ”€â”€ baseline_flux_bf16.png
    â”œâ”€â”€ onnx_output_{precision}.png
    â””â”€â”€ tensorrt_output_{precision}.png
```

## ğŸ”§ Requirements

### Hardware
- NVIDIA GPU (recommended 40GB+ VRAM, e.g., A100/A800)
- CUDA 12.0+
- TensorRT 10.0+

### Software
- Python 3.10+
- PyTorch 2.0+
- ONNX Runtime GPU 1.16+
- TensorRT 10.x

## ğŸš€ Quick Start

### 1. Install Dependencies

```bash
# Create virtual environment
conda create -n flux python=3.10
conda activate flux

# Install PyTorch (CUDA 12.x)
pip install torch torchvision torchaudio

# Install project dependencies
pip install -r requirements.txt

# Install ONNX Runtime GPU version
pip uninstall onnxruntime -y
pip install onnxruntime-gpu

# TensorRT is usually installed with CUDA, or download from NVIDIA website
```

### 2. Download FLUX.1-dev Model

```bash
huggingface-cli login
huggingface-cli download black-forest-labs/FLUX.1-dev --local-dir /path/to/models/FLUX.1-dev
```

### 3. Configure Model Path

Modify the `REPO_ROOT` variable in each file's `__main__`:

```python
REPO_ROOT = "/path/to/your/FLUX.1-dev"
```

## ğŸ“– Usage

### Baseline PyTorch Inference

```bash
python base_model.py
```

Output: `results/baseline_flux_bf16.png`

### ONNX Export and Inference

```bash
# Step 1: Export ONNX model (required for first time, ~5 minutes)
python onnx_model.py

# Step 2: Generate image with ONNX
python onnx_infer.py

# Or run benchmark only
python onnx_infer.py benchmark

# Check GPU support
python onnx_infer.py check
```

Output: `results/onnx_output_{precision}.png`

### TensorRT Accelerated Inference

```bash
# Step 1: Build TensorRT Engine (required for first time, ~5-10 minutes)
python tensorrt_model.py

# Step 2: Generate image with TensorRT
python tensorrt_infer.py

# Or run benchmark only
python tensorrt_infer.py benchmark
```

Output: `results/tensorrt_output_{precision}.png`

## âš™ï¸ Precision Configuration

Modify the `PRECISION` variable in each file's `__main__` function:

```python
PRECISION = "bf16"  # Options: "fp32", "fp16", "bf16"
```

### Precision Details

| Precision | ONNX | TensorRT | Description |
|-----------|:----:|:--------:|-------------|
| FP32 | âœ… | âŒ | Standard precision, high VRAM usage |
| FP16 | âœ… | âœ… | Recommended, fast |
| BF16 | âœ…* | âœ… | Converted to FP16 during export |
| INT8 | â³ | â³ | TODO: Requires calibration data |
| FP8 | âŒ | â³ | TODO: Requires Hopper GPU |

*BF16 is converted to FP16 during export due to limited ONNX Runtime BF16 support

## ğŸ“Š Performance Comparison

Test results based on NVIDIA A800 (80GB) (1024x1024, 28 steps):

| Method | Transformer Inference Time | Total Generation Time | VRAM Usage |
|--------|---------------------------|----------------------|------------|
| PyTorch (BF16) | ~350ms/step | ~12s | ~45GB |
| ONNX Runtime (FP16) | ~300ms/step | ~10s | ~40GB |
| TensorRT (FP16) | ~180ms/step | ~7s | ~35GB |

*Actual performance depends on specific hardware and configuration*

## â“ FAQ

### 1. ONNX Export Failed: `rms_norm` Not Supported

A custom `rms_norm` symbolic function has been registered in `onnx_model.py`.

### 2. TensorRT Cannot Find External Weights

Make sure the ONNX file and `*_weights.bin` are in the same directory.

### 3. ONNX Runtime Not Using GPU

```bash
# Make sure GPU version is installed
pip uninstall onnxruntime onnxruntime-gpu -y
pip install onnxruntime-gpu

# Run check
python onnx_infer.py check
```

### 4. Long TensorRT Build Time

First build takes 5-10 minutes, this is normal. Engine will be cached, subsequent loads are fast (~20s).

### 5. Out of Memory (OOM)

- Ensure you have enough GPU VRAM (40GB+ recommended)
- Close other GPU programs
- For TensorRT, adjust the `max_workspace_size` parameter

## ğŸ”— References

- [FLUX.1 Official Repo](https://github.com/black-forest-labs/flux)
- [Diffusers Documentation](https://huggingface.co/docs/diffusers)
- [ONNX Runtime Documentation](https://onnxruntime.ai/docs/)
- [TensorRT Documentation](https://docs.nvidia.com/deeplearning/tensorrt/)

## ğŸ“„ License

This project code is licensed under the MIT License.

FLUX.1-dev model usage is subject to its original license.

## ğŸ™ Acknowledgments

- **[Black Forest Labs](https://blackforestlabs.ai/)** - For open-sourcing FLUX.1 model
- **[Hugging Face](https://huggingface.co/)** - For Diffusers library and model hosting
- **[NVIDIA](https://nvidia.com/)** - For TensorRT and CUDA ecosystem
- **[GitHub Copilot](https://github.com/features/copilot)** - AI programming assistant that significantly improved development efficiency ğŸ¤–âœ¨

---

*Made with â¤ï¸ and GitHub Copilot*
